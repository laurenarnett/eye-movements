\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{pdfpages}

%%%%%%%%%
% Example of figure inclusion
%\begin{figure}[t]
%\begin{center}
%\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
%   %\includegraphics[width=0.8\linewidth]{egfigure.eps}
%\end{center}
%   \caption{Example of caption.  It is set in Roman so that mathematics
%   (always set in Roman: $B \sin A = A \sin B$) may be included without an
%   ugly clash.}
%\label{fig:long}
%\label{fig:onecol}
%\end{figure}

%%%%%%%%%
% Example table
%\noindent
%Compare the following:\\
%\begin{tabular}{ll}
% \verb'$conf_a$' &  $conf_a$ \\
% \verb'$\mathit{conf}_a$' & $\mathit{conf}_a$
%\end{tabular}\\
%
%
%
%\begin{table}
%\begin{center}
%\begin{tabular}{|l|c|}
%\hline
%Method & Frobnability \\
%\hline\hline
%Theirs & Frumpy \\
%Yours & Frobbly \\
%Ours & Makes one's heart Frob\\
%\hline
%\end{tabular}
%\end{center}
%\caption{Results.   Ours is better.}
%\end{table}

%%%%%%%%%
% another example figure
%\begin{figure*}
%\begin{center}
%\fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
%\end{center}
%   \caption{Example of a short caption, which should be centered.}
%\label{fig:short}
%\end{figure*}
%
%
%%%%%%%%%
% example citation
%   Frobnication has been trendy lately.
%   It was introduced by Alpher~\cite{Alpher02}, and subsequently developed by
%   Alpher and Fotheringham-Smythe~\cite{Alpher03}, and Alpher \etal~\cite{Alpher04}.''

    %Figure and table captions should be 9-point Roman type as in
    %Figures~\ref{fig:onecol} and~\ref{fig:short}.  Short captions should be centred.
% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
%
% When placing figures in \LaTeX, it's almost always best to use
% \verb+\includegraphics+, and to specify the  figure width as a multiple of
% the line width as in the example below
% {\small\begin{verbatim}
%    \usepackage[dvips]{graphicx} ...
%       \includegraphics[width=0.8\linewidth]
%           {myfile.eps}
%   \end{verbatim}
% }
%
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
\setcounter{page}{1}
\begin{document}

%%%%%%%%% TITLE
\title{A Predictive Model for Eye Fixations}

\author{Lauren Arnett and Chengzhi Mao\\
Columbia University in the City of New York\\
    {\tt\small \{lba2138,cm3797\}@columia.edu}
}

\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract} 
    
    Applications of tracking eye fixation location span from neuroscience and
    the study of human vision to advertising and human computer interaction. By
    creating a model to generate a prediction of where the eye may be most
    attracted to, industries can circumvent the expense of running studies on
    eye-tracking with actual human subjects. We look to improve upon existing
    models of saliency by using Bayesian methods with deep learning techniques.

\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction} It is estimated that 80\% of all external sensory input
processed by the human brain is processed by the visual pathway~\cite{Jerath}.
As such, optimizing image layout for processing by the human brain allows for
better information retrieval and retention across the image. Studying how
humansâ€™ eyes move across images is thus relevant for fields from neuroscience
to advertising and  art. Being able to predict where humans are most likely to
look provides a guideline as to whether the image has an effective layout, what
humans are attracted to in viewing art, or how an image should be cropped to
feature the subject. Using an existing dataset of eye movements, we build
a predictive model to generate the most likely fixation locations on a new
image.
%-------------------------------------------------------------------------
\section{Related Work} 

Traditionally, studies on eye movements have been carried out such that
viewers look at images on a monitor while an eye tracker records the
eye-fixations that stay within a threshold angle of movement. This procedure is
very costly, and necessitated formulating a method to predict where users will
look. Thus, models of saliency---the likelihood of a location to attract the
visual attention of a human---developed that are modeled mathematically using
biologically plausible linear filters. For example, linear combinations of
filters for low-level features such as color, intensity, and orientation
filters can be used to compute a total saliency map for an image,
providing a bottom-up understanding of the image~\cite{Itti}.

These models do not account for particular tasks that the viewer may have in
looking at the image, and often do not align with the ground truth fixation
locations. Judd \etal~\cite{Judd} propose using deep learning for this task
rather than deriving mathematical models and show that training from a large
database of eye-tracking data outperforms existing models. K\"ummerer
\etal~\cite{Kummerer} also employ deep learning for predicting fixation
locations, using the AlexNet architecture in \textit{DeepGaze I} and \textit{DeepGaze II}, built upon the VGG-19 network.

Developing a dataset for use by saliency models is also a field of exploration.
Judd \etal~\cite{Judd} creates a dataset of 1003 images with the fixation
locations from 15 viewers each and makes it publicly available. Jiang
\etal~\cite{Jiang} relies on an assumption of eye-mouse coordination---they
simulate recording eye-tracking data with instead recording mouse-tracking data
using Amazon Mechanical Turk. This provides a less expensive and
training-intensive method of developing a simulated saliency map. Finally, the
MIT300 dataset \cite{mitbench} looks to provide a performance benchmark for new predictive
models for saliency, with performance statistics for over 80 models at the time
of writing. 


\section{Dataset of Eye-Tracking Data}
\section{Learning a Model}
\subsection{Training}

\subsection{Performance}

\begin{figure*}
\begin{center}
\includegraphics[width=\columnwidth]{figures/tpr.pdf}

\end{center}
   \caption{Example of a short caption, which should be centered.}
\label{fig:short}
\end{figure*}

\section{Conclusion}

%------------------------------------------------------------------------
{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}

\end{document}
