\documentclass[11pt]{article}

\usepackage[pdftex]{graphicx}
\usepackage{url}

\usepackage[margin=1in]{geometry}

\begin{document}
%Title
\begin{center}
    \LARGE {A Predictive Model for Eye-Movements}
    \\\large Lauren Arnett (lba2138) \& Chengzhi Mao (cm3797)
    
\end{center}
\section{Project Statement} 
\vspace{-0.25cm}
It is estimated that human vision demands anywhere from 30 to 50 \% of brain
processing. As such, optimizing images for processing by the human brain allows
for better information retrieval and retention across the image. Applications
for study of how humans' eyes move across images span from advertising to art.
Being able to predict where humans are most likely to look provides a guideline
as to where to place the most important information, what humans are attracted
to in viewing art, or how an image should be cropped to feature the subject.
Using an existing dataset of eye movements, we are building a predictive model
to generate the most likely fixation locations on a new image. 
\vspace{-0.5cm}

\section{Methods} 

\vspace{-0.25cm}
We will be using one of two potential datasets of fixation locations. The
first, developed at Osnabr{\"u}ck University, aggregates 949 images
from multiple studies conducted with different batches of participants over
image categories of natural and urban scenes, web sites, fractal, pink-noise,
and ambiguous artistic figures.  The second was developed specifically for
a similar eye-movement-prediction study by researchers at MIT with data for 15
participants over 1003 natural images from Flickr and LabelMe. While the first
dataset includes a wider variety of images, the second dataset provides data
for many more images per viewer.

Regardless of which dataset we are using, we will have a relatively small
number of viewers per image. Given that we have this pre-defined population, we
are looking at using Bayesian methods to keep our model simple, in the spirit
of Occam's razor. Should we instead decide to use deep learning methods, we
will be exploring adversarial training methods to try to generalize our model.
% add some more specifics here

Specifically, we will to combine Bayesian inference with deep learning techniques,
since 1. the number of of training samples are limited and the eye movement labels are noisy, we need bayesian method to
inference under uncertainty. 2. the input image is high dimensional which traditional Bayesian methods are hard to handle,
deep learning methods are easy to process such data.

Adversarial training can be used to match the eye fixation distribution of different people, removing
individual specific information which make the model be able to generalize to unknown group of people.

Lauren will handle preprocessing of the dataset, Chengzhi will set up a testing
framework, and we will be working on building the model together. 
\vspace{-0.5cm}

\section{Evaluation}
We will use the standard 80\% / 20\% train / test split on our dataset,
reserving a subset of images for testing. Training on image data for all viewers of
an image, we will predict where that population is most likely to have fixation
locations on a new image. In order to validate how close we were to the actual
fixation location, we will be taking the distance between the means of the
actual and predicted coordinates. In addition, the fixiation point for different 
people looking at the same image is divisified, we would propose a variance groundtruth
as the indication of the uncertainty, which could further evaluate the uncertainty estimation
of our Bayesian Learning model.



\vspace{-0.25cm}


%Bibliography - TODO: need to make citation for MIT dataset
\begin{thebibliography}{99}
    \bibitem{Judd_2009}
      Judd, T., Ehinger, K., Durand, F. and Torralba, A.,
      (2009)
      ``Learning to Predict Where Humans Look'',
      IEEE International Conference on Computer Vision (ICCV),

    \bibitem{wilming2017} Wilming N., Onat S., Ossandón J., Acik A., Kietzmann
    T.C., Kaspar K., Gameiro R.R., Vormberg A., and König P., (2017)
    ``An extensive dataset of eye movements during viewing of complex
    images.'', Nature Scientific Data 4: 160126.  \verb|https://doi.org/10.1038/sdata.2016.126|
%example bibitem
%\bibitem{gonzalez2012} Jonay I. Gonz\'{a}lez Hern\'{a}ndez, 
%Pilar Ruiz-Lapuente,    
%Hugo M. Tabernero,    
%David Montes,    
%Ramon Canal,    
%Javier M\'{e}ndez    
%and Luigi R. Bedin,
%{No surviving evolved companions of the progenitor of SN1006},
%Nature, {\bf 489}, 533-536 (2012).

\end{thebibliography}



\end{document}
